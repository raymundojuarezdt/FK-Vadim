# -*- coding: utf-8 -*-
"""
Created on Fri February 20 2023

@author: RAYMUNDO JUÁREZ
"""

# PROGRAMA REALIZADO POR RAYMUNDO JUÁREZ PARA ARTÍCULO CON CARMEN BORREGO SALCIDO
#FILTRO DE KALMAN-VADIM APLICADO A SERIES DE TIEMPO MULTIVARIADO
#CON MODELO VAR
#MODELOS DE RIESGO SISTEMICO PARA EL SISTEMA BANCARIO MEXICANO
#COMPARACIÓN CON FILTRO DE KALMAN CLÁSICO
#EL MODELO FUE OBTENIDO USANDO EL SOFTWARE ECONOMÉTRICO EVIEWS 12
#
#RUIDO GAUSSIANO:
#EL COMPORTAMIENTO DE AMBOS FILTROS DEBE SER MUY SIMILAR

#RUIDO WEIBULL CON OUTLAYERS
#EL COMPORTAMIENTO DEL FK-VADIM DEBE
#ALCANZAR MEJOR LOS OUTLAYERS
#ACERCARSE MÁS A LAS OBSERVACIONES EN PUNTOS VERDES


#LOS VALORES CRÍTICOS QUE DEBES TENER EN CUENTA SON:

#CONDICIONES INCIALES DE XT Y YT: PRUEBA PRIMERO CAMBIAR LAS CONDICIONES DE YT
#LAS C.I. DEBEN AGRUPARSE: MODELO REAL (IDEAL) (100,0) Y OBSERVACIONES (RUIDO) (NA PARA i=0),
# Y ESTIMADOS  (95,1)

#LAS C.I. DEBE SER DIFERENTES PARA AMBOS GRUPOS, ESTO ES UN PUNTO IMPORTANTE

#VALORES PEQUEÑOS PARA QT Y HT
#ESTOS VALORES SON CONOCIDOS PARA EL CASO DE RUIDO GAUSSIANO
#ESTOS VALORES SON DESCONOCIDOS PARA OUTLAYERS 
#SUGERENCIA: DEJARLOS CON VALORES PEQUEÑOS

#VALOR DE P_EST
#ESTE VALOR DEBE SER PEQUEÑO O IGUAL AL VALOR DE QT CONOCIDO, PARA EL CASO 
#GAUSSIANO

#ESTE VALOR DEBE SER GRANDE PARA EL CASO DE RUIDO CON OUTLAYERS
#ESTE VALOR ES MUY IMPORTANTE PARA EL FUNCIONAMIENTO DEL FK-VADIM
#UN VALOR IADECUADO DE P_EST PROVOCA QUE FK-VADIM NO FUNCIONE


####################################################################################
####################################################################################
####################################################################################

#LIBRERIAS REQUERIDAS

import numpy as np
from numpy.linalg import inv
import matplotlib.pyplot as plt
import pandas as pd
import scipy.linalg as la

######################################################################################################
######################################################################################################

#VARIABLES DEPENDIENTES E INDEPENDIENTES EXTRAIDAS DE LA BASE DE DATOS:
#TASA BRUTA DE MIGRACIÓN
#PIB RS

# ESTAS LÍNEAS DE CÓDIGO SON PARA LLAMAR AL ARCHIVO EN EXCEL-SCV QUE CONTIENE
# LAS SERIES DE TIEMPO DEL MODELO ECONOMÉTRICO REALIZADO EN EVIEWS Y SE USARÁ
# PARA GENERAR LOS DATOS DEL VECTOR ACTUAL

x = pd.read_csv('D:/RESPALDO2023ABRIL/Investigacion2022/CarmenTesis/PRUEBA2.csv', delimiter=',', parse_dates=["Date"])
x1 =[x for x in x.SER_DESEMPLEO]
x1 = np.array(x1)
x2 =[x for x in x.SER_ICAP]
x2 = np.array(x2)
x3 =[x for x in x.SER_IGAE]
x3 = np.array(x3)
x4 =[x for x in x.SER_IMOR_DIF]
x4 = np.array(x4)
x5 =[x for x in x.SER_M1_DIF]
x5 = np.array(x5)
x6 =[x for x in x.SER_TDC_DIF2]
x6 = np.array(x6)
x7 =[x for x in x.SER_TIIE]
x7 = np.array(x7)
dat=[k for k in x.Date]
dat= np.array(dat)





######################################################################################
######################################################################################
######################################################################################

#IDENTIFICACIÓN DE DIMENSIONES DE LAS VARIABLES

#NÚMERO DE ESTADOS: ALPHA O SVi
nx=7
#DIMENSION DE OBSERVACIONES
nobs=dat.shape[0]
#NÚMERO DE VARIABLES DEPENDIENTES
ny=2


#####################################################################################
#####################################################################################
#####################################################################################

#DECLARACIÓN DE VARIABLES DEL MODELO EN EVIEWS

c11=0.923469
c12=0.047466
c13=-0.004898
c14=0.173422
c15=5.86E-9
c16=0.008795
c17=0.0039

c21=0.21733
c22=0.868855
c23=0.007307
c24=0.13683
c25=2.78E-9
c26=-0.048084
c27=0.064529

c31=-0.243498
c32=0.271863
c33=0.982835
c34=-2.703355
c35=-2.87E-8
c36=-1.1868
c37=-0.195627

c41=-0.002767
c42=0.003440
c43=-0.000784
c44=-0.023671
c45=-8.74E-10
c46=0.025610
c47=0.007855

c51=-746312.6
c52=-494301.6
c53=251054.9
c54=-10801455
c55=-0.004496
c56=172397.3
c57=-802530.3

c61=-0.163319
c62=0.067923
c63=0.001159
c64=0.225686
c65=-1.58E-8
c66=-0.187525
c67=-0.057479

c71=-0.115077
c72=0.024133
c73=0.003598
c74=-0.196639
c75=-3.89E-9
c76=0.014846
c77=0.961778


#VARIABLES:
#SER_DESEMPLEO
#SER_ICAP
#SER_IGAE
#SER_IMOR
#SER_M1
#SER_TDC
#SER_TIIE


# DECLARACIÓN DE VALORES CONSTANTES

Tt = np.array([
    [c11, c12, c13, c14, c15, c16, c17],
    [c21, c22, c23, c24, c25, c26, c27],
    [c31, c32, c33, c34, c35, c36, c37],
    [c41, c42, c43, c44, c45, c46, c47],
    [c51, c52, c53, c54, c55, c56, c57],
    [c61, c62, c63, c64, c65, c66, c67],
    [c71, c72, c73, c74, c75, c76, c77]])

Zt= np.array([
    [0, 1, 0, 0, 0, 0, 0],
    [0, 0, 0, 1, 0, 0, 0]])


# CONDICIONES INICIALES
#CI


#Ht = np.array(cov2)
Ht = 1E-6*np.array([0])
Qt = 1E-6*np.eye(nx)
#Qt = np.array(cov1)
#Qt = np.array([
#    [0.780446, 0, 0, 0, 0, 0, 0],
#    [0, 0.647662, 0, 0, 0, 0, 0],
#    [0, 0, 8.673122, 0, 0, 0, 0],
#    [0, 0, 0, 0.098017, 0, 0, 0],
#    [0, 0, 0, 0, 13327673, 0, 0],
#    [0, 0, 0, 0, 0, 0.638714, 0],
#    [0, 0, 0, 0, 0, 0, 1.780146]
#    ])
#Ht = np.array([
#    [0.647662, 0],
#    [0, 0.098017]])
P_init = 1E-1*np.eye(len(Qt))  # small initial prediction error
# matrix dimensions
nq = Qt.shape[0]


CIx=np.array([0,10,100,0.5,7000000,0,10])
CIx1=np.array([x1[0],x2[0],x3[0],x4[0],x5[0],x6[0],x7[0]])

yt=np.array([10,0.5])
yti1=np.array([x2[0],x4[0]])

x_init = np.array(CIx)
x_init1 = np.array(CIx1)

yt_init = np.array(yt)
yt_init1 = np.array(yti1)

#DECLARACIÓN DE CONTADORES PARA EXTENSIÓN CONVEXA DE FILTRO DE KALMAN (VADIM)
B=0
jmax=0
kmax=0
B1=0
jmax1=0
kmax1=0


##################################################################################
##################################################################################
##################################################################################

# DECLARACIÓN DE VARIABLES PARA ALGORITMO DE FK
# VARIABLES PARA FILTRO DE KALMAN
x_pred = np.zeros((nobs, nx))      # prediction of state vector
xpredmax = np.zeros((nobs, nx))      # prediction of state vector
P_pred = np.zeros((nobs, nq, nq))  # prediction error covariance matrix
Ppredmax = np.zeros((nobs, nq, nq))  # prediction error covariance matrix
x_est = np.zeros((nobs, nx))       # estimation of state vector
xestmax = np.zeros((nobs, nx))       # estimation of state vector
P_est = np.zeros((nobs, nq, nq))   # estimation error covariance matrix
Pestmax = np.zeros((nobs, nq, nq))   # estimation error covariance matrix
Pmax = np.zeros((nq, nq))   # estimation error covariance matrix
K = np.zeros((nobs, nq, ny))       # Kalman Gain
Kmax = np.zeros((nobs, nq, ny))       # Kalman Gain
#VARIABLES PARA EXTENSIÓN DE FILTRO DE KALMAN (VADIM)
x_pred1 = np.zeros((nobs, nx))      # prediction of state vector
xpredmax1 = np.zeros((nobs, nx))      # prediction of state vector
P_pred1 = np.zeros((nobs, nq, nq))  # prediction error covariance matrix
Ppredmax1 = np.zeros((nobs, nq, nq))  # prediction error covariance matrix
x_est1 = np.zeros((nobs, nx))       # estimation of state vector
xestmax1 = np.zeros((nobs, nx))       # estimation of state vector
P_est1 = np.zeros((nobs, nq, nq))   # estimation error covariance matrix
Pestmax1 = np.zeros((nobs, nq, nq))   # estimation error covariance matrix
Pmax1 = np.zeros((nq, nq))   # estimation error covariance matrix
K1 = np.zeros((nobs, nq, ny))       # Kalman Gain
Kmax1 = np.zeros((nobs, nq, ny))       # Kalman Gain
#VARIABLES PARA MODELO VAR
x_real = np.zeros((nobs, nx))      # prediction of state vector
x_real1 = np.zeros((nobs, nx))      # prediction of state vector
x_real2 = np.zeros((nobs, nx))      # prediction of state vector
yt_real = np.zeros((nobs, ny))       # estimation of state vector
yt_real1 = np.zeros((nobs, ny))       # estimation of state vector
yt_real2 = np.zeros((nobs, ny))       # estimation of state vector
yt_predV = np.zeros((nobs, ny))       # estimation of state vector
yt_predK = np.zeros((nobs, ny))       # estimation of state vector
yt_predV1 = np.zeros((nobs, ny))       # estimation of state vector
yt_predK1 = np.zeros((nobs, ny))       # estimation of state vector

xinit = np.zeros((nobs, nx)) 
yt = np.zeros((nobs, ny))       # estimation of state vector

####################################################################################
####################################################################################
####################################################################################

# CONDICIONES INICIALES PARA LAS VARIABLES DEL ALGORITMO DE FK
#FILTRO DE KALMAN CLÁSICO

x_pred[0] = x_init
xpredmax[0] = x_init
P_pred[0] = P_init
Ppredmax[0] = P_init
x_est[0] = x_init
P_est[0] = P_init
xestmax[0] = x_init
Pestmax[0] = P_init
Pmax = P_init
#EXTENSIÓN CONVEXA DEL FILTRO DE KALMAN (VADIM)
x_pred1[0] = x_init
xpredmax1[0] = x_init
P_pred1[0] = P_init
Ppredmax1[0] = P_init
x_est1[0] = x_init
P_est1[0] = P_init
xestmax1[0] = x_init
Pestmax1[0] = P_init
Pmax1 = P_init
#MODELO DEL VAR
x_real[0]=x_init1
x_real1[0]=x_init1
x_real2[0]=x_init1

yt_real[0] = yt_init1
yt_real1[0] = yt_init1
yt_real2[0] = yt_init1
yt_predV[0] = yt_init
yt_predK[0] = yt_init
yt_predV1[0] = yt_init
yt_predK1[0] = yt_init

######################################################################################
######################################################################################
#######################################################################################

# GENERACIÓN DE RUIDO PARA OBSERVACIONES

######################################################################################
######################################################################################
#######################################################################################


#CASO SIN RUIDO EN LAS VARIABLES
Ruix = np.zeros([nobs,nx])

#GENERACIÓN DE RUIDO EN LA VARIABLE DE SALIDA
#CASO EXTRAIDO DEL RUIDO ALFA
Ruiy = np.array(Ruix[:,0])
#CASO DE RUIDO ADICIONAL AL RUIDO ALFA
#mean2 = [0]
#cov2 = [[0.020]]  # diagonal covariance
#Ruiy = np.random.multivariate_normal(mean2, cov2, nobs).T
#Ruiy = np.zeros([nobs,ny])
#yweibul=0.3
#Ruialfa = np.random.weibull(yweibul, [nobs,ny]) 

#CREACIÓN DEL VECTOR DE OBSERVACIONES EN LAS VARIABLES
observationsx = np.array([x1,x2,x3,x4,x5,x6,x7]).T+Ruix
#CREACIÓN DEL VECTOR DE OBSERVACIONES EN LA SALIDA
observationsyt = np.array([observationsx[:,1],observationsx[:,3]]).T


################################################################################################################
################################################################################################################
################################################################################################################

#ALGORITMO DE FILTRO DE KALMAN

# MAIN CYCLE FOR KALMAN FILTER
for i in range(nobs):
# REAL STAGE
    if i > 0:
        #x_real1[i] = Tt @ x_real1[i-1]#+Ruix[i-1]
        x_real1[i] = Tt @ observationsx[i-1]#+Ruix[i-1]
        yt_real1[i] = Zt @ x_real1[i]#+Ruiy[i]
        
        if i > nobs-12:
            x_real[i] = Tt @ x_pred1[i-1]#+Ruix[i-1]
            yt_real[i] = Zt @ x_real[i]#+Ruiy[i]
        else:
            x_real[i] = Tt @ observationsx[i-1]#+Ruichi[i-1]
            yt_real[i] = Zt @ x_real[i]#+Ruiy[i]
        
        x_pred1[i] = Tt @ x_est1[i-1]
        P_pred1[i] = Tt @ P_est1[i-1] @ Tt.T + Qt
        yt_predK1[i]= Zt @ x_pred1[i]

# ESTIMATION STAGE
    K1[i] = P_pred1[i] @ Zt.T @ inv((Zt @ P_pred1[i] @ Zt.T)+ Ht)

    if i > nobs-12:
        x_est1[i] = x_pred1[i] + K1[i] @ (yt_real[i] - yt_predK1[i])
    else:
        x_est1[i] = x_pred1[i] + K1[i] @ (observationsyt[i] - yt_predK1[i])
    P_est1[i] = P_pred1[i] - K1[i] @ ((Zt @ P_pred1[i] @ Zt.T) + Ht) @ K1[i].T
    #P_est1[i] = P_pred1[i] - K1[i]  @ Zt @ P_pred1[i]
    
# KALMAN-VADIM GAIN MATRIX  STAGE 
    EigP1=la.eig(P_est1[i])
    sumEigP1=np.sum(EigP1[0])
    if sumEigP1>B1:
        B1=sumEigP1
        jmax1 +=1
        kmax1=i
        Pmax1=P_est1[i]
    if i==0:
        B1=sumEigP1
        Pmax1=P_init


################################################################################################################
################################################################################################################
################################################################################################################


#ALGORITMO DE EXTENSIÓN DE FILTRO DE KALMAN (VADIM)
#MAIN CYCLE FOR KALMAN FILTER EXTENSION BY VADIM AZHMYAKOV
for i in range(nobs):
    # prediction stage
    if i > 0:
        if i > nobs-12:
            x_real2[i] = Tt @ xpredmax1[i-1]#+Ruix[i-1]
            yt_real2[i] = Zt @ x_real2[i]#+Ruiy[i]
        else:
            x_real2[i] = Tt @ observationsx[i-1]#+Ruichi[i-1]
            yt_real2[i] = Zt @ x_real2[i]#+Ruiy[i]
        
        xpredmax1[i] = Tt @ xestmax1[i-1]
        Ppredmax1[i] = Tt @ Pestmax1[i-1] @ Tt.T + Qt
        yt_predV1[i]= Zt @ xpredmax1[i]
      
    # estimation stage
    Kmax1[i] = Ppredmax1[i] @ Zt.T @ inv((Zt @ Ppredmax1[i] @ Zt.T) + Ht)

    if i > nobs-12:
        xestmax1[i] = xpredmax1[i] + Kmax1[i]  @ (yt_real2[i] - yt_predV1[i])
    else:
        xestmax1[i] = xpredmax1[i] + Kmax1[i]  @ (observationsyt[i] - yt_predV1[i])
    Pestmax1[i] = Pmax1

################################################################################################################
################################################################################################################
################################################################################################################
#CALCULO DE ERRORES

#CALCULO DEL ERROR MEDIO CUADRÁTICO PARA LAS SERIES: FK CLÁSICO
diferenciaK1 = np.subtract(observationsyt,yt_predK1)
squaredK1 = np.square(diferenciaK1)
mseK1 = squaredK1.mean()
print(mseK1)

#CALCULO DEL ERROR MEDIO CUADRÁTICO PARA LAS SERIES: FK-VADIM
diferenciaV1 = np.subtract(observationsyt,yt_predV1)
squaredV1 = np.square(diferenciaV1)
mseV1 = squaredV1.mean()
print(mseV1)

################################################################################################################
################################################################################################################
################################################################################################################


#EXPORTAR ARREGLOS PARA ARCHIVO SCV
Obs=np.array(observationsyt)
yreal=np.array(yt_real1)
ytK=np.array(yt_predK1)
ytV=np.array(yt_predV1)
mseV=np.array(squaredV1)
mseK=np.array(squaredK1)
Carmen=np.concatenate((ytK,mseK,ytV,mseV,yreal,Obs),axis=1)
dfCarmen=pd.DataFrame(Carmen,columns=['ICAPK-12','IMORK-12','MseK1','MseK2',
                                          'ICAPV-12','IMORV-12','MseV1','MseV2',
                                          'ICAPVAR','IMORVAR','ObsICAP','ObsIMOR'])
dfCarmen.to_csv('CarmenVAR-FK12-Zero.csv')
       


################################################################################################################
################################################################################################################
################################################################################################################

# GRAFICA DE VARIABLES DE SALIDA


obsyt=np.append([[0,0]],observationsyt,axis=0)

plt.figure(figsize=(16,10))

with pd.plotting.plot_params.use("x_compat", True):
    #plt.scatter(dat, yt_real1[:], color="g", label='Calculated Obs')
    plt.plot(dat, yt_predK1[:,1], linestyle="-", color="r", label='FK')
    plt.plot(dat, yt_predV1[:,1], linestyle="-", color="k", label='FK-Vadim')
    plt.plot(dat, yt_real1[:,1], linestyle="-", color="g", label='VAR')
    #plt.plot(dat, yt_real[:], linestyle="-", color="b", label='VARKALMAN')
    #plt.plot(dat, yt_real2[:], linestyle="-", color="m", label='VARVADIM')
    plt.scatter(dat, obsyt[0:-1,1], linestyle="-", color="y", label='Original Data')

#plt.ylim([-0.0025,0.0025])
plt.xlabel('Time')
plt.ylabel('IMOR')
plt.title('Prediction IMOR')
plt.legend(loc=0)

plt.show()

# GRAFICA DE VARIABLES DE SALIDA CON ZOOM


plt.figure(figsize=(16,10))

with pd.plotting.plot_params.use("x_compat", True):
    #plt.scatter(dat, yt_real1[:], color="g", label='Calculated Obs')
    plt.plot(dat, yt_predK1[:,0], linestyle="-", color="r", label='FK')
    plt.plot(dat, yt_predV1[:,0], linestyle="-", color="k", label='FK-Vadim')
    plt.plot(dat, yt_real1[:,0], linestyle="-", color="g", label='VAR')
    #plt.plot(dat, yt_real[:], linestyle="-", color="b", label='VARKALMAN')
    #plt.plot(dat, yt_real2[:], linestyle="-", color="m", label='VARVADIM')
    plt.scatter(dat, obsyt[0:-1,0], linestyle="-", color="y", label='Original Data')

plt.ylim([10,20])
plt.xlabel('Time')
plt.ylabel('ICAP')
plt.title('PRediction ICAP')
plt.legend(loc=0)

plt.show()


# GRÁFICA DE ERRORES MEDIOS CUADRÁTICOS DE CADA ALGORITMO

plt.figure(figsize=(16,10))

with pd.plotting.plot_params.use("x_compat", True):
    plt.plot(dat, squaredK1[:,1], linestyle="-", color="r", label='FK')
    plt.plot(dat, squaredV1[:,1], linestyle="-", color="k", label='FK-V')

plt.xlabel('Time')
plt.ylabel('MSE with classic KF')
plt.title('Mean Square Error for Prediction IMOR')
plt.legend(loc=0)

plt.show()

plt.figure(figsize=(16,10))

with pd.plotting.plot_params.use("x_compat", True):
    plt.plot(dat, squaredK1[:,0], linestyle="-", color="r", label='FK')
    plt.plot(dat, squaredV1[:,0], linestyle="-", color="k", label='FK-V')

plt.xlabel('Time')
plt.ylabel('MSE with classic KF')
plt.title('Mean Square Error for Prediction ICAP')
plt.legend(loc=0)

plt.show()

######################################################################################
######################################################################################
#######################################################################################

# GENERACIÓN DE RUIDO PARA OBSERVACIONES: GAUSSIANAS

######################################################################################
######################################################################################
#######################################################################################

######################################################################################
######################################################################################
#######################################################################################

######################################################################################
######################################################################################
#######################################################################################

######################################################################################
######################################################################################
#######################################################################################
######################################################################################
######################################################################################
#######################################################################################

######################################################################################
######################################################################################
#######################################################################################
######################################################################################
######################################################################################
#######################################################################################

######################################################################################
######################################################################################
#######################################################################################


# CONDICIONES INICIALES
#CI


#Ht = np.array(cov2)
Ht = 1E-3*np.array([1])
Qt = 1E-6*np.eye(nx)
#Qt = np.array(cov1)
#Qt = np.array([
#    [0.780446, 0, 0, 0, 0, 0, 0],
#    [0, 0.647662, 0, 0, 0, 0, 0],
#    [0, 0, 8.673122, 0, 0, 0, 0],
#    [0, 0, 0, 0.098017, 0, 0, 0],
#    [0, 0, 0, 0, 13327673, 0, 0],
#    [0, 0, 0, 0, 0, 0.638714, 0],
#    [0, 0, 0, 0, 0, 0, 1.780146]
#    ])
#Ht = np.array([
#    [0.647662, 0],
#    [0, 0.098017]])
P_init = 1E3*np.eye(len(Qt))  # small initial prediction error


CIx=np.array([0,10,100,0.5,7000000,0,10])
CIx1=np.array([x1[0],x2[0],x3[0],x4[0],x5[0],x6[0],x7[0]])

yt=np.array([10,0.5])
yti1=np.array([x2[0],x4[0]])

x_init = np.array(CIx)
x_init1 = np.array(CIx1)

yt_init = np.array(yt)
yt_init1 = np.array(yti1)

#DECLARACIÓN DE CONTADORES PARA EXTENSIÓN CONVEXA DE FILTRO DE KALMAN (VADIM)
B=0
jmax=0
kmax=0
B1=0
jmax1=0
kmax1=0


##################################################################################
##################################################################################
##################################################################################

# DECLARACIÓN DE VARIABLES PARA ALGORITMO DE FK
# VARIABLES PARA FILTRO DE KALMAN
x_pred = np.zeros((nobs, nx))      # prediction of state vector
xpredmax = np.zeros((nobs, nx))      # prediction of state vector
P_pred = np.zeros((nobs, nq, nq))  # prediction error covariance matrix
Ppredmax = np.zeros((nobs, nq, nq))  # prediction error covariance matrix
x_est = np.zeros((nobs, nx))       # estimation of state vector
xestmax = np.zeros((nobs, nx))       # estimation of state vector
P_est = np.zeros((nobs, nq, nq))   # estimation error covariance matrix
Pestmax = np.zeros((nobs, nq, nq))   # estimation error covariance matrix
Pmax = np.zeros((nq, nq))   # estimation error covariance matrix
K = np.zeros((nobs, nq, ny))       # Kalman Gain
Kmax = np.zeros((nobs, nq, ny))       # Kalman Gain
#VARIABLES PARA EXTENSIÓN DE FILTRO DE KALMAN (VADIM)
x_pred1 = np.zeros((nobs, nx))      # prediction of state vector
xpredmax1 = np.zeros((nobs, nx))      # prediction of state vector
P_pred1 = np.zeros((nobs, nq, nq))  # prediction error covariance matrix
Ppredmax1 = np.zeros((nobs, nq, nq))  # prediction error covariance matrix
x_est1 = np.zeros((nobs, nx))       # estimation of state vector
xestmax1 = np.zeros((nobs, nx))       # estimation of state vector
P_est1 = np.zeros((nobs, nq, nq))   # estimation error covariance matrix
Pestmax1 = np.zeros((nobs, nq, nq))   # estimation error covariance matrix
Pmax1 = np.zeros((nq, nq))   # estimation error covariance matrix
K1 = np.zeros((nobs, nq, ny))       # Kalman Gain
Kmax1 = np.zeros((nobs, nq, ny))       # Kalman Gain
#VARIABLES PARA MODELO VAR
x_real = np.zeros((nobs, nx))      # prediction of state vector
x_real1 = np.zeros((nobs, nx))      # prediction of state vector
x_real2 = np.zeros((nobs, nx))      # prediction of state vector
yt_real = np.zeros((nobs, ny))       # estimation of state vector
yt_real1 = np.zeros((nobs, ny))       # estimation of state vector
yt_real2 = np.zeros((nobs, ny))       # estimation of state vector
yt_predV = np.zeros((nobs, ny))       # estimation of state vector
yt_predK = np.zeros((nobs, ny))       # estimation of state vector
yt_predV1 = np.zeros((nobs, ny))       # estimation of state vector
yt_predK1 = np.zeros((nobs, ny))       # estimation of state vector

xinit = np.zeros((nobs, nx)) 
yt = np.zeros((nobs, ny))       # estimation of state vector

####################################################################################
####################################################################################
####################################################################################

# CONDICIONES INICIALES PARA LAS VARIABLES DEL ALGORITMO DE FK
#FILTRO DE KALMAN CLÁSICO

x_pred[0] = x_init
xpredmax[0] = x_init
P_pred[0] = P_init
Ppredmax[0] = P_init
x_est[0] = x_init
P_est[0] = P_init
xestmax[0] = x_init
Pestmax[0] = P_init
Pmax = P_init
#EXTENSIÓN CONVEXA DEL FILTRO DE KALMAN (VADIM)
x_pred1[0] = x_init
xpredmax1[0] = x_init
P_pred1[0] = P_init
Ppredmax1[0] = P_init
x_est1[0] = x_init
P_est1[0] = P_init
xestmax1[0] = x_init
Pestmax1[0] = P_init
Pmax1 = P_init
#MODELO DEL VAR
x_real[0]=x_init1
x_real1[0]=x_init1
x_real2[0]=x_init1

yt_real[0] = yt_init1
yt_real1[0] = yt_init1
yt_real2[0] = yt_init1
yt_predV[0] = yt_init
yt_predK[0] = yt_init
yt_predV1[0] = yt_init
yt_predK1[0] = yt_init

######################################################################################
######################################################################################
#######################################################################################


#CASO CON RUIDO GAUSSIANO
mean1 = [0,0,0,0,0,0,0]
cov1 = [[0.02, 0, 0, 0, 0, 0, 0], [0,0.01, 0, 0, 0, 0, 0], [0,0, 0.04, 0, 0, 0, 0],
        [0,0, 0, 0.01, 0, 0, 0], [0,0, 0, 0, 0.02, 0, 0], [0,0, 0, 0, 0, 0.01, 0],
        [0,0, 0, 0, 0, 0, 0.04]]  # diagonal covariance
Rx1, Rx2, Rx3, Rx4, Rx5, Rx6, Rx7 = np.random.multivariate_normal(mean1, cov1, nobs).T
Ruix = np.array([Rx1,Rx2,Rx3,Rx4,Rx5,Rx6,Rx7]).T

#GENERACIÓN DE RUIDO EN LA VARIABLE DE SALIDA
#CASO EXTRAIDO DEL RUIDO ALFA
Ruiy = np.array(Ruix[:,0])
#CASO DE RUIDO ADICIONAL AL RUIDO ALFA
#mean2 = [0]
#cov2 = [[0.020]]  # diagonal covariance
#Ruiy = np.random.multivariate_normal(mean2, cov2, nobs).T
#Ruiy = np.zeros([nobs,ny])
#yweibul=0.3
#Ruialfa = np.random.weibull(yweibul, [nobs,ny]) 

#CREACIÓN DEL VECTOR DE OBSERVACIONES EN LAS VARIABLES
observationsx = np.array([x1,x2,x3,x4,x5,x6,x7]).T+Ruix
#CREACIÓN DEL VECTOR DE OBSERVACIONES EN LA SALIDA
observationsyt = np.array([observationsx[:,1],observationsx[:,3]]).T


################################################################################################################
################################################################################################################
################################################################################################################

#ALGORITMO DE FILTRO DE KALMAN

# MAIN CYCLE FOR KALMAN FILTER
for i in range(nobs):
# REAL STAGE
    if i > 0:
        #x_real1[i] = Tt @ x_real1[i-1]#+Ruix[i-1]
        x_real1[i] = Tt @ observationsx[i-1]#+Ruix[i-1]
        yt_real1[i] = Zt @ x_real1[i]#+Ruiy[i]
        
        if i > nobs-12:
            x_real[i] = Tt @ x_pred1[i-1]#+Ruix[i-1]
            yt_real[i] = Zt @ x_real[i]#+Ruiy[i]
        else:
            x_real[i] = Tt @ observationsx[i-1]#+Ruichi[i-1]
            yt_real[i] = Zt @ x_real[i]#+Ruiy[i]
        
        x_pred1[i] = Tt @ x_est1[i-1]
        P_pred1[i] = Tt @ P_est1[i-1] @ Tt.T + Qt
        yt_predK1[i]= Zt @ x_pred1[i]

# ESTIMATION STAGE
    K1[i] = P_pred1[i] @ Zt.T @ inv((Zt @ P_pred1[i] @ Zt.T)+ Ht)

    if i > nobs-12:
        x_est1[i] = x_pred1[i] + K1[i] @ (yt_real[i] - yt_predK1[i])
    else:
        x_est1[i] = x_pred1[i] + K1[i] @ (observationsyt[i] - yt_predK1[i])
    P_est1[i] = P_pred1[i] - K1[i] @ ((Zt @ P_pred1[i] @ Zt.T) + Ht) @ K1[i].T
    #P_est1[i] = P_pred1[i] - K1[i]  @ Zt @ P_pred1[i]
    
# KALMAN-VADIM GAIN MATRIX  STAGE 
    EigP1=la.eig(P_est1[i])
    sumEigP1=np.sum(EigP1[0])
    if sumEigP1>B1:
        B1=sumEigP1
        jmax1 +=1
        kmax1=i
        Pmax1=P_est1[i]
    if i==0:
        B1=sumEigP1
        Pmax1=P_init


################################################################################################################
################################################################################################################
################################################################################################################


#ALGORITMO DE EXTENSIÓN DE FILTRO DE KALMAN (VADIM)
#MAIN CYCLE FOR KALMAN FILTER EXTENSION BY VADIM AZHMYAKOV
for i in range(nobs):
    # prediction stage
    if i > 0:
        if i > nobs-12:
            x_real2[i] = Tt @ xpredmax1[i-1]#+Ruix[i-1]
            yt_real2[i] = Zt @ x_real2[i]#+Ruiy[i]
        else:
            x_real2[i] = Tt @ observationsx[i-1]#+Ruichi[i-1]
            yt_real2[i] = Zt @ x_real2[i]#+Ruiy[i]
        
        xpredmax1[i] = Tt @ xestmax1[i-1]
        Ppredmax1[i] = Tt @ Pestmax1[i-1] @ Tt.T + Qt
        yt_predV1[i]= Zt @ xpredmax1[i]
      
    # estimation stage
    Kmax1[i] = Ppredmax1[i] @ Zt.T @ inv((Zt @ Ppredmax1[i] @ Zt.T) + Ht)

    if i > nobs-12:
        xestmax1[i] = xpredmax1[i] + Kmax1[i]  @ (yt_real2[i] - yt_predV1[i])
    else:
        xestmax1[i] = xpredmax1[i] + Kmax1[i]  @ (observationsyt[i] - yt_predV1[i])
    Pestmax1[i] = Pmax1

################################################################################################################
################################################################################################################
################################################################################################################
#CALCULO DE ERRORES

#CALCULO DEL ERROR MEDIO CUADRÁTICO PARA LAS SERIES: FK CLÁSICO
diferenciaK1 = np.subtract(observationsyt,yt_predK1)
squaredK1 = np.square(diferenciaK1)
mseK1 = squaredK1.mean()
print(mseK1)

#CALCULO DEL ERROR MEDIO CUADRÁTICO PARA LAS SERIES: FK-VADIM
diferenciaV1 = np.subtract(observationsyt,yt_predV1)
squaredV1 = np.square(diferenciaV1)
mseV1 = squaredV1.mean()
print(mseV1)

################################################################################################################
################################################################################################################
################################################################################################################


#EXPORTAR ARREGLOS PARA ARCHIVO SCV
Obs=np.array(observationsyt)
yreal=np.array(yt_real1)
ytK=np.array(yt_predK1)
ytV=np.array(yt_predV1)
mseV=np.array(squaredV1)
mseK=np.array(squaredK1)
Carmen1=np.concatenate((ytK,mseK,ytV,mseV,yreal,Obs),axis=1)
dfCarmen1=pd.DataFrame(Carmen1,columns=['ICAPK-12','IMORK-12','MseK1','MseK2',
                                          'ICAPV-12','IMORV-12','MseV1','MseV2',
                                          'ICAPVAR','IMORVAR','ObsICAP','ObsIMOR'])
dfCarmen1.to_csv('CarmenVAR-FK12-Gauss.csv')
       


################################################################################################################
################################################################################################################
################################################################################################################

# GRAFICA DE VARIABLES DE SALIDA


obsyt=np.append([[0,0]],observationsyt,axis=0)

plt.figure(figsize=(16,10))

with pd.plotting.plot_params.use("x_compat", True):
    #plt.scatter(dat, yt_real1[:], color="g", label='Calculated Obs')
    plt.plot(dat, yt_predK1[:,1], linestyle="-", color="r", label='FK')
    plt.plot(dat, yt_predV1[:,1], linestyle="-", color="k", label='FK-Vadim')
    plt.plot(dat, yt_real1[:,1], linestyle="-", color="g", label='VAR')
    #plt.plot(dat, yt_real[:], linestyle="-", color="b", label='VARKALMAN')
    #plt.plot(dat, yt_real2[:], linestyle="-", color="m", label='VARVADIM')
    plt.scatter(dat, obsyt[0:-1,1], linestyle="-", color="y", label='Original Data')

#plt.ylim([-0.0025,0.0025])
plt.xlabel('Time')
plt.ylabel('IMOR')
plt.title('Prediction IMOR')
plt.legend(loc=0)

plt.show()

# GRAFICA DE VARIABLES DE SALIDA CON ZOOM


plt.figure(figsize=(16,10))

with pd.plotting.plot_params.use("x_compat", True):
    #plt.scatter(dat, yt_real1[:], color="g", label='Calculated Obs')
    plt.plot(dat, yt_predK1[:,0], linestyle="-", color="r", label='FK')
    plt.plot(dat, yt_predV1[:,0], linestyle="-", color="k", label='FK-Vadim')
    plt.plot(dat, yt_real1[:,0], linestyle="-", color="g", label='VAR')
    #plt.plot(dat, yt_real[:], linestyle="-", color="b", label='VARKALMAN')
    #plt.plot(dat, yt_real2[:], linestyle="-", color="m", label='VARVADIM')
    plt.scatter(dat, obsyt[0:-1,0], linestyle="-", color="y", label='Original Data')

plt.ylim([10,20])
plt.xlabel('Time')
plt.ylabel('ICAP')
plt.title('Prediction ICAP')
plt.legend(loc=0)

plt.show()


# GRÁFICA DE ERRORES MEDIOS CUADRÁTICOS DE CADA ALGORITMO

plt.figure(figsize=(16,10))

with pd.plotting.plot_params.use("x_compat", True):
    plt.plot(dat, squaredK1[:,1], linestyle="-", color="r", label='FK')
    plt.plot(dat, squaredV1[:,1], linestyle="-", color="k", label='FK-V')

plt.xlabel('Time')
plt.ylabel('MSE with classic KF')
plt.title('Mean Square Error for Prediction IMOR')
plt.legend(loc=0)

plt.show()

plt.figure(figsize=(16,10))

with pd.plotting.plot_params.use("x_compat", True):
    plt.plot(dat, squaredK1[:,0], linestyle="-", color="r", label='FK')
    plt.plot(dat, squaredV1[:,0], linestyle="-", color="k", label='FK-V')

plt.xlabel('Time')
plt.ylabel('MSE with classic KF')
plt.title('Mean Square Error for Prediction ICAP')
plt.legend(loc=0)

plt.show()

######################################################################################
######################################################################################
#######################################################################################

# GENERACIÓN DE RUIDO PARA OBSERVACIONES: OUTLAYERS

######################################################################################
######################################################################################
#######################################################################################


######################################################################################
######################################################################################
#######################################################################################


######################################################################################
######################################################################################
#######################################################################################

######################################################################################
######################################################################################
#######################################################################################

######################################################################################
######################################################################################
#######################################################################################
######################################################################################
######################################################################################
#######################################################################################

######################################################################################
######################################################################################
#######################################################################################
######################################################################################
######################################################################################
#######################################################################################

######################################################################################
######################################################################################
#######################################################################################



# CONDICIONES INICIALES
#CI

#Ht = np.array(cov2)
Ht = 1E-6*np.array([1])
Qt = 1E-6*np.eye(nx)
#Qt = np.array([
#    [0.780446, 0, 0, 0, 0, 0, 0],
#    [0, 0.647662, 0, 0, 0, 0, 0],
#    [0, 0, 8.673122, 0, 0, 0, 0],
#    [0, 0, 0, 0.098017, 0, 0, 0],
#    [0, 0, 0, 0, 13327673, 0, 0],
#    [0, 0, 0, 0, 0, 0.638714, 0],
#    [0, 0, 0, 0, 0, 0, 1.780146]
#    ])
#Ht = np.array([
#    [0.647662, 0],
#    [0, 0.098017]])
#Qt = np.array(cov1)
P_init = 1E-1*np.eye(len(Qt))  # small initial prediction error


CIx=np.array([0,10,100,0.5,7000000,0,10])
CIx1=np.array([x1[0],x2[0],x3[0],x4[0],x5[0],x6[0],x7[0]])

yt=np.array([10,0.5])
yti1=np.array([x2[0],x4[0]])

x_init = np.array(CIx)
x_init1 = np.array(CIx1)

yt_init = np.array(yt)
yt_init1 = np.array(yti1)

#DECLARACIÓN DE CONTADORES PARA EXTENSIÓN CONVEXA DE FILTRO DE KALMAN (VADIM)
B=0
jmax=0
kmax=0
B1=0
jmax1=0
kmax1=0


##################################################################################
##################################################################################
##################################################################################

# DECLARACIÓN DE VARIABLES PARA ALGORITMO DE FK
# VARIABLES PARA FILTRO DE KALMAN
x_pred = np.zeros((nobs, nx))      # prediction of state vector
xpredmax = np.zeros((nobs, nx))      # prediction of state vector
P_pred = np.zeros((nobs, nq, nq))  # prediction error covariance matrix
Ppredmax = np.zeros((nobs, nq, nq))  # prediction error covariance matrix
x_est = np.zeros((nobs, nx))       # estimation of state vector
xestmax = np.zeros((nobs, nx))       # estimation of state vector
P_est = np.zeros((nobs, nq, nq))   # estimation error covariance matrix
Pestmax = np.zeros((nobs, nq, nq))   # estimation error covariance matrix
Pmax = np.zeros((nq, nq))   # estimation error covariance matrix
K = np.zeros((nobs, nq, ny))       # Kalman Gain
Kmax = np.zeros((nobs, nq, ny))       # Kalman Gain
#VARIABLES PARA EXTENSIÓN DE FILTRO DE KALMAN (VADIM)
x_pred1 = np.zeros((nobs, nx))      # prediction of state vector
xpredmax1 = np.zeros((nobs, nx))      # prediction of state vector
P_pred1 = np.zeros((nobs, nq, nq))  # prediction error covariance matrix
Ppredmax1 = np.zeros((nobs, nq, nq))  # prediction error covariance matrix
x_est1 = np.zeros((nobs, nx))       # estimation of state vector
xestmax1 = np.zeros((nobs, nx))       # estimation of state vector
P_est1 = np.zeros((nobs, nq, nq))   # estimation error covariance matrix
Pestmax1 = np.zeros((nobs, nq, nq))   # estimation error covariance matrix
Pmax1 = np.zeros((nq, nq))   # estimation error covariance matrix
K1 = np.zeros((nobs, nq, ny))       # Kalman Gain
Kmax1 = np.zeros((nobs, nq, ny))       # Kalman Gain
#VARIABLES PARA MODELO VAR
x_real = np.zeros((nobs, nx))      # prediction of state vector
x_real1 = np.zeros((nobs, nx))      # prediction of state vector
x_real2 = np.zeros((nobs, nx))      # prediction of state vector
yt_real = np.zeros((nobs, ny))       # estimation of state vector
yt_real1 = np.zeros((nobs, ny))       # estimation of state vector
yt_real2 = np.zeros((nobs, ny))       # estimation of state vector
yt_predV = np.zeros((nobs, ny))       # estimation of state vector
yt_predK = np.zeros((nobs, ny))       # estimation of state vector
yt_predV1 = np.zeros((nobs, ny))       # estimation of state vector
yt_predK1 = np.zeros((nobs, ny))       # estimation of state vector

xinit = np.zeros((nobs, nx)) 
yt = np.zeros((nobs, ny))       # estimation of state vector

####################################################################################
####################################################################################
####################################################################################

# CONDICIONES INICIALES PARA LAS VARIABLES DEL ALGORITMO DE FK
#FILTRO DE KALMAN CLÁSICO

x_pred[0] = x_init
xpredmax[0] = x_init
P_pred[0] = P_init
Ppredmax[0] = P_init
x_est[0] = x_init
P_est[0] = P_init
xestmax[0] = x_init
Pestmax[0] = P_init
Pmax = P_init
#EXTENSIÓN CONVEXA DEL FILTRO DE KALMAN (VADIM)
x_pred1[0] = x_init
xpredmax1[0] = x_init
P_pred1[0] = P_init
Ppredmax1[0] = P_init
x_est1[0] = x_init
P_est1[0] = P_init
xestmax1[0] = x_init
Pestmax1[0] = P_init
Pmax1 = P_init
#MODELO DEL VAR
x_real[0]=x_init1
x_real1[0]=x_init1
x_real2[0]=x_init1

yt_real[0] = yt_init1
yt_real1[0] = yt_init1
yt_real2[0] = yt_init1
yt_predV[0] = yt_init
yt_predK[0] = yt_init
yt_predV1[0] = yt_init
yt_predK1[0] = yt_init

######################################################################################
######################################################################################
#######################################################################################


#CASO RUIDO CON OUTLAYERS
aweibul=0.3
Ruix = np.random.weibull(aweibul, [nobs,nx]) 


#GENERACIÓN DE RUIDO EN LA VARIABLE DE SALIDA
#CASO EXTRAIDO DEL RUIDO ALFA
Ruiy = np.array(Ruix[:,0])
#CASO DE RUIDO ADICIONAL AL RUIDO ALFA
#mean2 = [0]
#cov2 = [[0.020]]  # diagonal covariance
#Ruiy = np.random.multivariate_normal(mean2, cov2, nobs).T
#Ruiy = np.zeros([nobs,ny])
#yweibul=0.3
#Ruialfa = np.random.weibull(yweibul, [nobs,ny]) 

#CREACIÓN DEL VECTOR DE OBSERVACIONES EN LAS VARIABLES
observationsx = np.array([x1,x2,x3,x4,x5,x6,x7]).T+Ruix
#CREACIÓN DEL VECTOR DE OBSERVACIONES EN LA SALIDA
observationsyt = np.array([observationsx[:,1],observationsx[:,3]]).T


################################################################################################################
################################################################################################################
################################################################################################################

#ALGORITMO DE FILTRO DE KALMAN

# MAIN CYCLE FOR KALMAN FILTER
for i in range(nobs):
# REAL STAGE
    if i > 0:
        #x_real1[i] = Tt @ x_real1[i-1]#+Ruix[i-1]
        x_real1[i] = Tt @ observationsx[i-1]#+Ruix[i-1]
        yt_real1[i] = Zt @ x_real1[i]#+Ruiy[i]
        
        if i > nobs-12:
            x_real[i] = Tt @ x_pred1[i-1]#+Ruix[i-1]
            yt_real[i] = Zt @ x_real[i]#+Ruiy[i]
        else:
            x_real[i] = Tt @ observationsx[i-1]#+Ruichi[i-1]
            yt_real[i] = Zt @ x_real[i]#+Ruiy[i]
        
        x_pred1[i] = Tt @ x_est1[i-1]
        P_pred1[i] = Tt @ P_est1[i-1] @ Tt.T + Qt
        yt_predK1[i]= Zt @ x_pred1[i]

# ESTIMATION STAGE
    K1[i] = P_pred1[i] @ Zt.T @ inv((Zt @ P_pred1[i] @ Zt.T)+ Ht)

    if i > nobs-12:
        x_est1[i] = x_pred1[i] + K1[i] @ (yt_real[i] - yt_predK1[i])
    else:
        x_est1[i] = x_pred1[i] + K1[i] @ (observationsyt[i] - yt_predK1[i])
    P_est1[i] = P_pred1[i] - K1[i] @ ((Zt @ P_pred1[i] @ Zt.T) + Ht) @ K1[i].T
    #P_est1[i] = P_pred1[i] - K1[i]  @ Zt @ P_pred1[i]
    
# KALMAN-VADIM GAIN MATRIX  STAGE 
    EigP1=la.eig(P_est1[i])
    sumEigP1=np.sum(EigP1[0])
    if sumEigP1>B1:
        B1=sumEigP1
        jmax1 +=1
        kmax1=i
        Pmax1=P_est1[i]
    if i==0:
        B1=sumEigP1
        Pmax1=P_init


################################################################################################################
################################################################################################################
################################################################################################################


#ALGORITMO DE EXTENSIÓN DE FILTRO DE KALMAN (VADIM)
#MAIN CYCLE FOR KALMAN FILTER EXTENSION BY VADIM AZHMYAKOV
for i in range(nobs):
    # prediction stage
    if i > 0:
        if i > nobs-12:
            x_real2[i] = Tt @ xpredmax1[i-1]#+Ruix[i-1]
            yt_real2[i] = Zt @ x_real2[i]#+Ruiy[i]
        else:
            x_real2[i] = Tt @ observationsx[i-1]#+Ruichi[i-1]
            yt_real2[i] = Zt @ x_real2[i]#+Ruiy[i]
        
        xpredmax1[i] = Tt @ xestmax1[i-1]
        Ppredmax1[i] = Tt @ Pestmax1[i-1] @ Tt.T + Qt
        yt_predV1[i]= Zt @ xpredmax1[i]
      
    # estimation stage
    Kmax1[i] = Ppredmax1[i] @ Zt.T @ inv((Zt @ Ppredmax1[i] @ Zt.T) + Ht)

    if i > nobs-12:
        xestmax1[i] = xpredmax1[i] + Kmax1[i]  @ (yt_real2[i] - yt_predV1[i])
    else:
        xestmax1[i] = xpredmax1[i] + Kmax1[i]  @ (observationsyt[i] - yt_predV1[i])
    Pestmax1[i] = Pmax1

################################################################################################################
################################################################################################################
################################################################################################################
#CALCULO DE ERRORES

#CALCULO DEL ERROR MEDIO CUADRÁTICO PARA LAS SERIES: FK CLÁSICO
diferenciaK1 = np.subtract(observationsyt,yt_predK1)
squaredK1 = np.square(diferenciaK1)
mseK1 = squaredK1.mean()
print(mseK1)

#CALCULO DEL ERROR MEDIO CUADRÁTICO PARA LAS SERIES: FK-VADIM
diferenciaV1 = np.subtract(observationsyt,yt_predV1)
squaredV1 = np.square(diferenciaV1)
mseV1 = squaredV1.mean()
print(mseV1)

################################################################################################################
################################################################################################################
################################################################################################################


#EXPORTAR ARREGLOS PARA ARCHIVO SCV
Obs=np.array(observationsyt)
yreal=np.array(yt_real1)
ytK=np.array(yt_predK1)
ytV=np.array(yt_predV1)
mseV=np.array(squaredV1)
mseK=np.array(squaredK1)
Carmen2=np.concatenate((ytK,mseK,ytV,mseV,yreal,Obs),axis=1)
dfCarmen2=pd.DataFrame(Carmen2,columns=['ICAPK-12','IMORK-12','MseK1','MseK2',
                                          'ICAPV-12','IMORV-12','MseV1','MseV2',
                                          'ICAPVAR','IMORVAR','ObsICAP','ObsIMOR'])
dfCarmen2.to_csv('CarmenVAR-FK12-Outlayer.csv')
       


################################################################################################################
################################################################################################################
################################################################################################################

# GRAFICA DE VARIABLES DE SALIDA


obsyt=np.append([[0,0]],observationsyt,axis=0)

plt.figure(figsize=(16,10))

with pd.plotting.plot_params.use("x_compat", True):
    #plt.scatter(dat, yt_real1[:], color="g", label='Calculated Obs')
    plt.plot(dat, yt_predK1[:,1], linestyle="-", color="r", label='FK')
    plt.plot(dat, yt_predV1[:,1], linestyle="-", color="k", label='FK-Vadim')
    plt.plot(dat, yt_real1[:,1], linestyle="-", color="g", label='VAR')
    #plt.plot(dat, yt_real[:], linestyle="-", color="b", label='VARKALMAN')
    #plt.plot(dat, yt_real2[:], linestyle="-", color="m", label='VARVADIM')
    plt.scatter(dat, obsyt[0:-1,1], linestyle="-", color="y", label='Original Data')

#plt.ylim([-0.0025,0.0025])
plt.xlabel('Time')
plt.ylabel('IMOR')
plt.title('Prediction IMOR')
plt.legend(loc=0)

plt.show()


# GRAFICA DE VARIABLES DE SALIDA CON ZOOM


plt.figure(figsize=(16,10))

with pd.plotting.plot_params.use("x_compat", True):
    #plt.scatter(dat, yt_real1[:], color="g", label='Calculated Obs')
    plt.plot(dat, yt_predK1[:,0], linestyle="-", color="r", label='FK')
    plt.plot(dat, yt_predV1[:,0], linestyle="-", color="k", label='FK-Vadim')
    plt.plot(dat, yt_real1[:,0], linestyle="-", color="g", label='VAR')
    #plt.plot(dat, yt_real[:], linestyle="-", color="b", label='VARKALMAN')
    #plt.plot(dat, yt_real2[:], linestyle="-", color="m", label='VARVADIM')
    plt.scatter(dat, obsyt[0:-1,0], linestyle="-", color="y", label='Original Data')

#plt.ylim([-2,10])
plt.xlabel('Time')
plt.ylabel('ICAP')
plt.title('Prediction ICAP')
plt.legend(loc=0)

plt.show()

# GRÁFICA DE ERRORES MEDIOS CUADRÁTICOS DE CADA ALGORITMO

plt.figure(figsize=(16,10))

with pd.plotting.plot_params.use("x_compat", True):
    plt.plot(dat, squaredK1[:], linestyle="-", color="r", label='FK')
    plt.plot(dat, squaredV1[:], linestyle="-", color="k", label='FK-V')

plt.xlabel('Time')
plt.ylabel('MSE with classic KF')
plt.title('Mean Square Error for Prediction IMOR')
plt.legend(loc=0)

plt.show()

plt.figure(figsize=(16,10))

with pd.plotting.plot_params.use("x_compat", True):
    plt.plot(dat, squaredK1[:], linestyle="-", color="r", label='FK')
    plt.plot(dat, squaredV1[:], linestyle="-", color="k", label='FK-V')

plt.xlabel('Time')
plt.ylabel('MSE with classic KF')
plt.title('Mean Square Error for Prediction ICAP')
plt.legend(loc=0)

plt.show()
